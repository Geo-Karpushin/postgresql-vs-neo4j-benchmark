#!/usr/bin/env python3
"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤.
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞.
"""

import subprocess
import sys
import time
import json
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any

DATA_DIR = Path("generated")
SCRIPTS_DIR = Path("scripts")
RESULTS_DIR = Path("results")
POSTGRES_CONTAINER = "database-benchmark-postgres-1"
NEO4J_CONTAINER = "database-benchmark-neo4j-1"
DOCKER_RETRIES = 4
DOCKER_BACKOFF = 2

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
DATASETS_CONFIG = {
    "small": {
        "users": 50_000,
        "avg_friends": 20,
        "iterations": 5,
        "query_runs": {
            "simple_friends": 50,
            "friends_of_friends": 400,
            "mutual_friends": 50,
            "friend_recommendations": 20,
            "shortest_path": 5
        }
    },
    "medium": {
        "users": 500_000,
        "avg_friends": 15,
        "iterations": 3,
        "query_runs": {
            "simple_friends": 30,
            "friends_of_friends": 300,
            "mutual_friends": 30,
            "friend_recommendations": 15,
            "shortest_path": 3
        }
    },
    "large": {
        "users": 2_000_000,
        "avg_friends": 12,
        "iterations": 2,
        "query_runs": {
            "simple_friends": 20,
            "friends_of_friends": 250,
            "mutual_friends": 20,
            "friend_recommendations": 10,
            "shortest_path": 2
        }
    },
    "x-large": {
        "users": 5_000_000,
        "avg_friends": 10,
        "iterations": 1,
        "query_runs": {
            "simple_friends": 10,
            "friends_of_friends": 200,
            "mutual_friends": 10,
            "friend_recommendations": 5,
            "shortest_path": 1
        }
    },
    "xx-large": {
        "users": 10_000_000,
        "avg_friends": 8,
        "iterations": 1,
        "query_runs": {
            "simple_friends": 5,
            "friends_of_friends": 100,
            "mutual_friends": 5,
            "friend_recommendations": 3,
            "shortest_path": 1
        }
    }
}

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
log = logging.getLogger("dataset_manager")

def run_cmd(cmd: List[str], capture: bool = True, check: bool = True) -> subprocess.CompletedProcess:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥ (–±–µ–∑ shell=True)."""
    return subprocess.run(cmd, text=True, capture_output=capture, check=check)

def retry_cmd(cmd: List[str], retries: int = DOCKER_RETRIES, backoff: int = DOCKER_BACKOFF) -> bool:
    """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–º–∞–Ω–¥—É —Å retry/backoff. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –ø—Ä–∏ —É—Å–ø–µ—Ö–µ, False –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ."""
    attempt = 0
    delay = backoff
    while attempt < retries:
        try:
            run_cmd(cmd)
            return True
        except subprocess.CalledProcessError as e:
            attempt += 1
            log.warning("–ö–æ–º–∞–Ω–¥–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å (–ø–æ–ø—ã—Ç–∫–∞ %d/%d): %s ‚Äî %s", attempt, retries, " ".join(cmd), e.stderr.strip()[:200])
            if attempt < retries:
                time.sleep(delay)
                delay *= 2
    return False

class DatasetManager:
    def __init__(self, dry_run: bool = False):
        self.base_path = DATA_DIR
        self.scripts_path = SCRIPTS_DIR
        self.results_path = RESULTS_DIR
        self.dry_run = dry_run
        self.results_path.mkdir(parents=True, exist_ok=True)
        self.config = DATASETS_CONFIG

    def _ensure_dataset_files(self, size: str) -> bool:
        users = self.base_path / size / "users.csv"
        friendships = self.base_path / size / "friendships.csv"
        if not users.exists():
            log.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª: %s", users)
            return False
        if not friendships.exists():
            log.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª: %s", friendships)
            return False
        return True

    def initialize_databases(self) -> bool:
        log.info("üóÉÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
        try:
            run_cmd([sys.executable, str(self.scripts_path / "init_database.py")])
            log.info("‚úÖ –°—Ö–µ–º—ã –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: %s", e.stderr.strip())
            return False
        
    def cleanup_databases(self) -> bool:
        log.info("üßπ –û—á–∏—Å—Ç–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
        try:
            subprocess.run(
                [sys.executable, str(self.scripts_path / "cleanup_databases.py")],
                check=True
            )
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: %s", e)
            return False

    def generate_dataset(self, size: str) -> bool:
        log.info("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ %s...", size)
        try:
            config = self.config.get(size, {})
            subprocess.run(
                [sys.executable, str(self.scripts_path / "data_generator.py"), 
                str(config.get("users", 50000)), 
                str(config.get("avg_friends", 15)), 
                size],
                check=True
            )
            log.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç %s —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω", size)
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", e)
            return False

    def copy_to_containers(self, size: str) -> bool:
        log.info("üì¶ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ %s –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã...", size)
        if not self._ensure_dataset_files(size):
            return False

        users_host = str(self.base_path / size / "users.csv")
        friends_host = str(self.base_path / size / "friendships.csv")

        cp_pg_users = ["docker", "cp", users_host, f"{POSTGRES_CONTAINER}:/tmp/users.csv"]
        cp_pg_friends = ["docker", "cp", friends_host, f"{POSTGRES_CONTAINER}:/tmp/friendships.csv"]

        neo4j_dir = f"/var/lib/neo4j/import/{size}"
        mkdir_neo = ["docker", "exec", NEO4J_CONTAINER, "mkdir", "-p", neo4j_dir]
        cp_neo_users = ["docker", "cp", users_host, f"{NEO4J_CONTAINER}:{neo4j_dir}/users.csv"]
        cp_neo_friends = ["docker", "cp", friends_host, f"{NEO4J_CONTAINER}:{neo4j_dir}/friendships.csv"]

        steps = [
            (mkdir_neo, "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ Neo4j"),
            (cp_pg_users, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ users -> Postgres"),
            (cp_pg_friends, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ friendships -> Postgres"),
            (cp_neo_users, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ users -> Neo4j"),
            (cp_neo_friends, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ friendships -> Neo4j"),
        ]

        for cmd, desc in steps:
            log.info("  ‚Ä¢ %s: %s", desc, " ".join(cmd) if self.dry_run else "")
            if self.dry_run:
                continue
            ok = retry_cmd(cmd)
            if not ok:
                log.error("  ‚ùå –û—à–∏–±–∫–∞ —à–∞–≥–∞: %s", desc)
                return False

        log.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç %s —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã", size)
        return True

    def load_to_databases(self, size: str) -> bool:
        log.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ %s –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –±–∞–∑—ã...", size)

        loader = self.scripts_path / "load_data.py"
        if not loader.exists():
            log.error("–°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", loader)
            return False

        try:
            subprocess.run(
                [sys.executable, str(loader), size],
                check=True
            )
            log.info("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –±–∞–∑—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True

        except subprocess.CalledProcessError:
            log.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
            return False

    def run_benchmarks(self, size: str) -> bool:
        log.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è %s...", size)

        runner = self.scripts_path / "benchmark_runner.py"
        if not runner.exists():
            log.error("–°–∫—Ä–∏–ø—Ç –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", runner)
            return False

        try:
            config = self.config.get(size, {})
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª –¥–ª—è query_runs
            config_file = self.results_path / f"benchmark_config_{size}.json"
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config.get("query_runs", {}), f, indent=2)
            
            subprocess.run(
                [sys.executable, str(runner), size,
                 "--config", str(config_file)],
                check=True
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª
            config_file.unlink(missing_ok=True)
            
            log.info("‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è %s –∑–∞–≤–µ—Ä—à–µ–Ω—ã", size)
            return True

        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: %s", e)
            return False

    def process_size(self, size: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞: –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ —à–∞–≥–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."""
        result = {"size": size, "timestamps": {}, "durations": {}, "status": "unknown", "config": self.config.get(size, {})}

        ok = self.cleanup_databases()
        if not ok:
            result["status"] = "cleanup_failed"
            return result
        
        ok = self.initialize_databases()
        if not ok:
            log.error("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º –Ω–µ —É–¥–∞–ª–∞—Å—å. –í—ã—Ö–æ–¥.")
            return result

        start = time.time()
        t0 = time.time()
        ok = self.generate_dataset(size)
        result["timestamps"]["generate_start"] = t0
        result["durations"]["generate"] = time.time() - t0
        if not ok:
            result["status"] = "generate_failed"
            return result

        t1 = time.time()
        ok = self.copy_to_containers(size)
        result["timestamps"]["copy_start"] = t1
        result["durations"]["copy"] = time.time() - t1
        if not ok:
            result["status"] = "copy_failed"
            return result

        t2 = time.time()
        ok = self.load_to_databases(size)
        result["timestamps"]["load_start"] = t2
        result["durations"]["load"] = time.time() - t2
        if not ok:
            result["status"] = "load_failed"
            return result

        t3 = time.time()
        ok = self.run_benchmarks(size)
        result["timestamps"]["benchmark_start"] = t3
        result["durations"]["benchmark"] = time.time() - t3
        if not ok:
            result["status"] = "bench_failed"
            return result
        
        ok = self.cleanup_databases()
        if not ok:
            result["status"] = "cleanup_failed"
            return result

        result["status"] = "ok"
        result["total_time"] = time.time() - start
        return result

def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python dataset_manager.py [small / medium / large / x-large / xx-large / all] [--dry-run]")
        return

    target = sys.argv[1]
    dry = "--dry-run" in sys.argv

    manager = DatasetManager(dry_run=dry)

    sizes = ["small", "medium", "large", "x-large", "xx-large"] if target == "all" else [target]

    for size in sizes:
        log.info("=" * 60)
        log.info("üéØ –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–¢–ê–°–ï–¢–ê: %s", size.upper())
        log.info("üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: %s –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, %s —Å—Ä–µ–¥–Ω–∏—Ö –¥—Ä—É–∑–µ–π", 
                manager.config.get(size, {}).get("users", "N/A"),
                manager.config.get(size, {}).get("avg_friends", "N/A"))
        
        oks = 0
        for i in range(1, DATASETS_CONFIG[size]["iterations"]+1):
            log.info("üîÑ –ò–¢–ï–†–ê–¶–ò–Ø %i/%i", i, DATASETS_CONFIG[size]["iterations"])
            res = manager.process_size(size)
            out_file = manager.results_path / f"{size}.json"
            with open(out_file, "w", encoding="utf-8") as f:
                json.dump(res, f, ensure_ascii=False, indent=2)
            log.info("–°–æ—Ö—Ä–∞–Ω—ë–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç: %s", out_file)
            if res["status"] != "ok":
                oks += 1
                log.warning("–û–±—Ä–∞–±–æ—Ç–∫–∞ %s –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å —Å—Ç–∞—Ç—É—Å–æ–º: %s", size, res["status"])
            else:
                log.info("üéâ %s –¥–∞—Ç–∞—Å–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω! (–≤—Ä–µ–º—è %.2fs)", size, res["total_time"])
        
        log.info("–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–¢–ê–°–ï–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê, —É—Å–ø–µ—à–Ω–æ: %i/%i", oks, DATASETS_CONFIG[size]["iterations"])

if __name__ == "__main__":
    main()
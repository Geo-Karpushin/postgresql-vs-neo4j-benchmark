#!/usr/bin/env python3
"""
–£–º–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏, –∞–Ω–∞–ª–∏–∑–æ–º —Ç—Ä–µ–Ω–¥–æ–≤
–∏ —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π –ø—Ä–∏ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏ —Ä–µ—Å—É—Ä—Å–æ–≤ (poor, medium, rich).
"""

import subprocess
import sys
import time
import json
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime
import statistics
from scipy import stats
import numpy as np

BASE_DIR = Path(__file__).parent.parent.resolve()  # –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
DATA_DIR = BASE_DIR / "generated"
SCRIPTS_DIR = BASE_DIR / "scripts"
RESULTS_DIR = BASE_DIR / "results"
POSTGRES_CONTAINER = "database-benchmark-postgres-1"
NEO4J_CONTAINER = "database-benchmark-neo4j-1"
DOCKER_RETRIES = 4
DOCKER_BACKOFF = 2

# –£–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É
ORDERED_SIZES = [
    "super-tiny",
    "tiny", 
    "very-small",
    "small",
    "medium",
    "large",
    "x-large",
    "xx-large"
]

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–æ—Ç –±–µ–¥–Ω–æ–π –∫ –±–æ–≥–∞—Ç–æ–π)
CONFIGS = ["poor", "medium", "rich"]

# –ë–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–±—É–¥—É—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è)
DATASETS_CONFIG = {
    "super-tiny": {
        "users": 5_000,
        "avg_friends": 5,
        "iterations": 3,
        "query_runs": {
            "simple_friends": 50,
            "friends_of_friends": 100,
            "mutual_friends": 50,
            "friend_recommendations": 30,
            "shortest_path": 10,
            "cohort_analysis": 10,
            "social_cities": 10,
            "age_gap_analysis": 10,
            "network_growth": 3,
            "age_clustering": 3
        }
    },
    "tiny": {
        "users": 10_000,
        "avg_friends": 22,
        "iterations": 3,
        "query_runs": {
            "simple_friends": 40,
            "friends_of_friends": 80,
            "mutual_friends": 40,
            "friend_recommendations": 25,
            "shortest_path": 8,
            "cohort_analysis": 8,
            "social_cities": 8,
            "age_gap_analysis": 8,
            "network_growth": 2,
            "age_clustering": 2
        }
    },
    "very-small": {
        "users": 20_000,
        "avg_friends": 500,
        "iterations": 3,
        "query_runs": {
            "simple_friends": 30,
            "friends_of_friends": 60,
            "mutual_friends": 30,
            "friend_recommendations": 20,
            "shortest_path": 6,
            "cohort_analysis": 6,
            "social_cities": 6,
            "age_gap_analysis": 6,
            "network_growth": 4,
            "age_clustering": 4
        }
    },
    "small": {
        "users": 50_000,
        "avg_friends": 20,
        "iterations": 2,
        "query_runs": {
            "simple_friends": 25,
            "friends_of_friends": 50,
            "mutual_friends": 25,
            "friend_recommendations": 15,
            "shortest_path": 5,
            "cohort_analysis": 5,
            "social_cities": 5,
            "age_gap_analysis": 5,
            "network_growth": 2,
            "age_clustering": 2
        }
    },
    "medium": {
        "users": 100_000,
        "avg_friends": 50,
        "iterations": 2,
        "query_runs": {
            "simple_friends": 20,
            "friends_of_friends": 40,
            "mutual_friends": 20,
            "friend_recommendations": 12,
            "shortest_path": 4,
            "cohort_analysis": 4,
            "social_cities": 4,
            "age_gap_analysis": 4,
            "network_growth": 2,
            "age_clustering": 2
        }
    },
    "large": {
        "users": 250_000,
        "avg_friends": 15,
        "iterations": 1,
        "query_runs": {
            "simple_friends": 15,
            "friends_of_friends": 30,
            "mutual_friends": 15,
            "friend_recommendations": 10,
            "shortest_path": 3,
            "cohort_analysis": 3,
            "social_cities": 3,
            "age_gap_analysis": 3,
            "network_growth": 2,
            "age_clustering": 2
        }
    },
    "x-large": {
        "users": 500_000,
        "avg_friends": 12,
        "iterations": 1,
        "query_runs": {
            "simple_friends": 10,
            "friends_of_friends": 20,
            "mutual_friends": 10,
            "friend_recommendations": 8,
            "shortest_path": 3,
            "cohort_analysis": 3,
            "social_cities": 3,
            "age_gap_analysis": 3,
            "network_growth": 2,
            "age_clustering": 2
        }
    },
    "xx-large": {
        "users": 1_000_000,
        "avg_friends": 100,
        "iterations": 1,
        "query_runs": {
            "simple_friends": 5,
            "friends_of_friends": 5,
            "mutual_friends": 5,
            "friend_recommendations": 5,
            "shortest_path": 5,
            "cohort_analysis": 5,
            "social_cities": 5,
            "age_gap_analysis": 5,
            "network_growth": 5,
            "age_clustering": 5
        }
    }
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def setup_logging(config_name: str = "all"):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = f"testing_{config_name}_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s: %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger()

class TrendAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç—Ä–µ–Ω–¥–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    @staticmethod
    def calculate_efficiency_coefficient(pg_time: float, neo_time: float) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–±–æ–ª—å—à–µ 1 = Neo4j –±—ã—Å—Ç—Ä–µ–µ)"""
        if pg_time <= 0 or neo_time <= 0:
            return 1.0
        return neo_time / pg_time
    
    @staticmethod
    def analyze_benchmark_result(result_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        efficiency = result_data.get("efficiency", {})
        if not efficiency:
            return {}
        
        summary = efficiency.get("_summary", {})
        if not summary:
            return {}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
        tests_analysis = {}
        for test_name, test_data in efficiency.items():
            if test_name.startswith("_"):
                continue
            
            tests_analysis[test_name] = {
                "efficiency_coefficient": test_data.get("efficiency_coefficient", 1.0),
                "improvement_percentage": test_data.get("improvement_percentage", 0),
                "postgres_time_ms": test_data.get("postgres_time_ms", 0),
                "neo4j_time_ms": test_data.get("neo4j_time_ms", 0),
                "significance": test_data.get("significance", "—Å—Ä–µ–¥–Ω—è—è")
            }
        
        return {
            "summary": {
                "average_efficiency": summary.get("average_efficiency", 1.0),
                "median_efficiency": summary.get("median_efficiency", 1.0),
                "neo4j_wins_count": summary.get("neo4j_wins_count", 0),
                "postgres_wins_count": summary.get("postgres_wins_count", 0),
                "total_comparisons": summary.get("total_comparisons", 0),
                "overall_winner": summary.get("overall_winner", "None"),
                "performance_advantage": summary.get("performance_advantage", "0%")
            },
            "tests": tests_analysis
        }
    
    @staticmethod
    def analyze_trends(efficiency_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        if len(efficiency_history) < 2:
            return {"has_trend": False, "trend": "insufficient_data"}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        avg_efficiencies = []
        median_efficiencies = []
        neo_wins_counts = []
        pg_wins_counts = []
        
        for hist in efficiency_history:
            if "summary" in hist:
                summary = hist["summary"]
                avg_efficiencies.append(summary.get("average_efficiency", 1.0))
                median_efficiencies.append(summary.get("median_efficiency", 1.0))
                neo_wins_counts.append(summary.get("neo4j_wins_count", 0))
                pg_wins_counts.append(summary.get("postgres_wins_count", 0))
        
        if len(avg_efficiencies) < 2:
            return {"has_trend": False, "trend": "insufficient_data"}
        
        try:
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ —Å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π
            x = list(range(len(avg_efficiencies)))
            y = avg_efficiencies
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ–º –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å scipy
            try:
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
                has_significant_trend = p_value < 0.1
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç—Ä–µ–Ω–¥–∞
                if abs(slope) < 0.05:
                    trend = "stable"
                elif slope > 0.1:
                    trend = "neo4j_improving"
                elif slope > 0:
                    trend = "neo4j_slightly_improving"
                elif slope < -0.1:
                    trend = "postgres_improving"
                else:
                    trend = "postgres_slightly_improving"
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–±–µ–¥—ã
                neo_wins_trend = "increasing" if neo_wins_counts[-1] > neo_wins_counts[0] else "decreasing"
                pg_wins_trend = "increasing" if pg_wins_counts[-1] > pg_wins_counts[0] else "decreasing"
                
                # –ê–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
                volatility = np.std(y) / np.mean(y) if len(y) > 1 else 0
                
                return {
                    "has_trend": has_significant_trend,
                    "trend": trend,
                    "slope": float(slope),
                    "r_squared": float(r_value**2),
                    "p_value": float(p_value),
                    "volatility": float(volatility),
                    "efficiency_range": (float(min(y)), float(max(y))),
                    "current_efficiency": float(y[-1]) if y else 1.0,
                    "neo_wins_trend": neo_wins_trend,
                    "pg_wins_trend": pg_wins_trend,
                    "data_points": len(y)
                }
                
            except ImportError:
                # Fallback –±–µ–∑ scipy
                # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑: —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –∏–ª–∏ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                if len(y) >= 3:
                    first_half = statistics.mean(y[:len(y)//2])
                    second_half = statistics.mean(y[len(y)//2:])
                    slope_est = (second_half - first_half) / (len(y) // 2)
                    
                    if slope_est > 0.05:
                        trend = "neo4j_improving"
                    elif slope_est < -0.05:
                        trend = "postgres_improving"
                    else:
                        trend = "stable"
                    
                    return {
                        "has_trend": True,
                        "trend": trend,
                        "slope": float(slope_est),
                        "volatility": float(np.std(y) / np.mean(y) if np.mean(y) > 0 else 0),
                        "efficiency_range": (float(min(y)), float(max(y))),
                        "current_efficiency": float(y[-1]) if y else 1.0,
                        "data_points": len(y)
                    }
                else:
                    return {"has_trend": False, "trend": "insufficient_data"}
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
            return {"has_trend": False, "trend": "analysis_error"}
    
    @staticmethod
    def should_stop_based_on_trend(trend_analysis: Dict[str, Any], current_size: str) -> Tuple[bool, str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–¥–∞"""
        if not trend_analysis.get("has_trend", False):
            return False, "–ù–µ—Ç –∑–Ω–∞—á–∏–º–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞"
        
        trend = trend_analysis.get("trend", "stable")
        current_eff = trend_analysis.get("current_efficiency", 1.0)
        slope = trend_analysis.get("slope", 0)
        volatility = trend_analysis.get("volatility", 0)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞
        if "large" in current_size or "x-large" in current_size:
            stop_threshold = 0.1  # –ë–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            confidence_threshold = 0.8  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        else:
            stop_threshold = 0.15
            confidence_threshold = 0.7
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        
        # 1. Neo4j —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–µ—Ç –∏ —Ç—Ä–µ–Ω–¥ —É—Ö—É–¥—à–∞–µ—Ç—Å—è
        if current_eff < 0.5 and slope < -stop_threshold and volatility < 0.2:
            return True, f"Neo4j —Å–∏–ª—å–Ω–æ –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–µ—Ç (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {current_eff:.2f}) –∏ —Ç—Ä–µ–Ω–¥ —É—Ö—É–¥—à–∞–µ—Ç—Å—è"
        
        # 2. PostgreSQL —Å—Ç–∞–±–∏–ª—å–Ω–æ –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç –∏ —É–ª—É—á—à–∞–µ—Ç—Å—è
        if current_eff > 2.0 and slope > stop_threshold and volatility < 0.2:
            return True, f"PostgreSQL —Å–∏–ª—å–Ω–æ –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {current_eff:.2f}) –∏ —É–ª—É—á—à–∞–µ—Ç—Å—è"
        
        # 3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å —Å –±–æ–ª—å—à–∏–º –æ—Ç—Ä—ã–≤–æ–º
        if volatility < 0.1 and abs(slope) < stop_threshold:
            if current_eff < 0.7:
                return True, f"Neo4j —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–µ—Ç (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {current_eff:.2f}, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {volatility:.2f})"
            elif current_eff > 1.5:
                return True, f"PostgreSQL —Å—Ç–∞–±–∏–ª—å–Ω–æ –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {current_eff:.2f}, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {volatility:.2f})"
        
        # 4. –†–∞–∑—Ä—ã–≤ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ
        if abs(slope) > 0.3 and volatility > 0.3:
            direction = "–≤ –ø–æ–ª—å–∑—É Neo4j" if slope > 0 else "–≤ –ø–æ–ª—å–∑—É PostgreSQL"
            return True, f"–†–∞–∑—Ä—ã–≤ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è {direction} (–Ω–∞–∫–ª–æ–Ω: {slope:.2f})"
        
        return False, "–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"

class AdaptiveQueryManager:
    """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ —É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–≥–æ–Ω–æ–≤ —Ç–µ—Å—Ç–æ–≤"""
    
    def __init__(self, base_config: Dict[str, Dict[str, Any]]):
        self.base_config = base_config
        self.results_history: Dict[str, Dict[str, Dict[str, Any]]] = {}
        self.test_performance_history: Dict[str, List[float]] = {}
        
    def update_from_results(self, size: str, result_data: Dict[str, Any]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
        if "efficiency" not in result_data:
            return
        
        efficiency_data = result_data["efficiency"]
        self.results_history[size] = efficiency_data
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Å—Ç–∞
        for test_name, test_data in efficiency_data.items():
            if test_name.startswith("_"):
                continue
            
            eff_coeff = test_data.get("efficiency_coefficient", 1.0)
            if test_name not in self.test_performance_history:
                self.test_performance_history[test_name] = []
            
            self.test_performance_history[test_name].append(eff_coeff)
    
    def get_adaptive_config(self, size: str, previous_size: str = None) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–≥–æ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏"""
        base_runs = self.base_config.get(size, {}).get("query_runs", {})
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏ –∏–ª–∏ –ø–µ—Ä–≤—ã–π —Ä–∞–∑–º–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if not self.results_history or previous_size not in self.results_history:
            print(f"–ò—Å–ø–æ–ª—å–∑—É—é –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞ {size}")
            return base_runs.copy()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        prev_results = self.results_history.get(previous_size, {})
        
        adaptive_runs = {}
        
        for test_name, base_run_count in base_runs.items():
            test_data = prev_results.get(test_name, {})
            eff_coeff = test_data.get("efficiency_coefficient", 1.0)
            significance = test_data.get("significance", "—Å—Ä–µ–¥–Ω—è—è")
            improvement = test_data.get("improvement_percentage", 0)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è —Ç–µ—Å—Ç–∞
            if test_name in self.test_performance_history:
                history = self.test_performance_history[test_name]
                if len(history) >= 2:
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥ —Ç–µ—Å—Ç–∞
                    trend = "improving" if history[-1] > history[-2] else "worsening"
                    volatility = np.std(history[-min(3, len(history)):]) / np.mean(history[-min(3, len(history)):]) if len(history) >= 2 else 0
                else:
                    trend = "unknown"
                    volatility = 0
            else:
                trend = "unknown"
                volatility = 0
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
            
            # –ü—Ä–∞–≤–∏–ª–æ 1: –û—á–µ–Ω—å –ø–ª–æ—Ö–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Neo4j
            if eff_coeff < 0.3 and significance == "–≤—ã—Å–æ–∫–∞—è":
                # –°–æ–∫—Ä–∞—â–∞–µ–º –ø—Ä–æ–≥–æ–Ω—ã –≤ 6 —Ä–∞–∑
                new_runs = max(2, base_run_count // 6)
                reason = "–û—á–µ–Ω—å –ø–ª–æ—Ö–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Neo4j"
            
            # –ü—Ä–∞–≤–∏–ª–æ 2: –ü–ª–æ—Ö–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Neo4j
            elif eff_coeff < 0.6:
                # –°–æ–∫—Ä–∞—â–∞–µ–º –ø—Ä–æ–≥–æ–Ω—ã –≤ 3 —Ä–∞–∑–∞
                new_runs = max(3, base_run_count // 3)
                reason = "–ü–ª–æ—Ö–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Neo4j"
            
            # –ü—Ä–∞–≤–∏–ª–æ 3: –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Neo4j
            elif eff_coeff > 2.0 and significance == "–≤—ã—Å–æ–∫–∞—è":
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–æ–≥–æ–Ω—ã –≤ 2 —Ä–∞–∑–∞ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                new_runs = min(100, base_run_count * 2)
                reason = "–û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Neo4j"
            
            # –ü—Ä–∞–≤–∏–ª–æ 4: –•–æ—Ä–æ—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Neo4j
            elif eff_coeff > 1.3:
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–æ–≥–æ–Ω—ã
                new_runs = min(80, int(base_run_count * 1.5))
                reason = "–•–æ—Ä–æ—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Neo4j"
            
            # –ü—Ä–∞–≤–∏–ª–æ 5: –ë–æ–ª—å—à–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è (–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
            elif abs(improvement) > 300 and volatility > 0.4:
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–æ–≥–æ–Ω—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
                new_runs = min(60, int(base_run_count * 1.3))
                reason = "–í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
            
            # –ü—Ä–∞–≤–∏–ª–æ 6: –¢–µ—Å—Ç —Å —Ä–∞—Å—Ç—É—â–∏–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º Neo4j
            elif trend == "improving" and eff_coeff > 0.8:
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø—Ä–æ–≥–æ–Ω—ã –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
                new_runs = min(70, int(base_run_count * 1.4))
                reason = "–†–∞—Å—Ç—É—â–µ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ Neo4j"
            
            # –ü—Ä–∞–≤–∏–ª–æ 7: –°—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            elif volatility < 0.2 and abs(improvement) < 100:
                # –°–ª–µ–≥–∫–∞ —É–º–µ–Ω—å—à–∞–µ–º –ø—Ä–æ–≥–æ–Ω—ã
                new_runs = max(5, int(base_run_count * 0.8))
                reason = "–°—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
            
            else:
                # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                new_runs = base_run_count
                reason = "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
            
            adaptive_runs[test_name] = new_runs
            
            if new_runs != base_run_count:
                print(f"  –¢–µ—Å—Ç {test_name}: {base_run_count} ‚Üí {new_runs} –ø—Ä–æ–≥–æ–Ω–æ–≤ ({reason})")
        
        return adaptive_runs

class AdaptiveTestingManager:
    """–£–º–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
    
    def __init__(self, config_name: str = "all", dry_run: bool = False):
        self.config_name = config_name
        self.base_path = DATA_DIR
        self.scripts_path = SCRIPTS_DIR
        self.results_path = RESULTS_DIR / config_name
        self.dry_run = dry_run
        self.results_path.mkdir(parents=True, exist_ok=True)
        
        self.config = DATASETS_CONFIG
        self.trend_analyzer = TrendAnalyzer()
        self.query_manager = AdaptiveQueryManager(DATASETS_CONFIG)
        
        # –ò—Å—Ç–æ—Ä–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.efficiency_history: List[Dict[str, Any]] = []
        self.size_results: Dict[str, List[Dict[str, Any]]] = {}
        self.testing_log: List[Dict[str, Any]] = []
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_iterations": 0,
            "successful_iterations": 0,
            "failed_iterations": 0,
            "total_time": 0,
            "sizes_completed": [],
            "adaptations_applied": 0
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.log = setup_logging(config_name)
    
    def run_cmd(self, cmd: List[str], capture: bool = False, check: bool = True) -> subprocess.CompletedProcess:
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥"""
        if self.dry_run:
            self.log.info(f"DRY RUN: {' '.join(cmd)}")
            return subprocess.CompletedProcess(cmd, 0, "", "")
        return subprocess.run(cmd, text=True, capture_output=capture, check=check)
    
    def retry_cmd(self, cmd: List[str], retries: int = DOCKER_RETRIES, backoff: int = DOCKER_BACKOFF) -> bool:
        """–ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥—ã —Å backoff"""
        for attempt in range(retries):
            try:
                self.run_cmd(cmd)
                return True
            except subprocess.CalledProcessError:
                if attempt < retries - 1:
                    time.sleep(backoff * (2 ** attempt))
        return False
    
    def initialize_databases(self, infrastructure_config: str) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""
        self.log.info(f"üóÉÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {infrastructure_config})...")
        try:
            self.run_cmd([sys.executable, str(self.scripts_path / "init_database.py"), "init", infrastructure_config])
            self.log.info("‚úÖ –°—Ö–µ–º—ã –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            return True
        except subprocess.CalledProcessError as e:
            self.log.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: %s", e.stderr.strip())
            return False
    
    def cleanup_databases(self, infrastructure_config: str) -> bool:
        """–û—á–∏—Å—Ç–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""
        self.log.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {infrastructure_config})...")
        try:
            self.run_cmd([
                sys.executable, str(self.scripts_path / "cleanup_databases.py"),
                "--config", infrastructure_config
            ])
            return True
        except subprocess.CalledProcessError as e:
            self.log.error("‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: %s", e)
            return False
    
    def generate_dataset(self, size: str) -> bool:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        self.log.info("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ %s...", size)
        try:
            config = self.config.get(size, {})
            self.run_cmd([
                sys.executable, str(self.scripts_path / "data_generator.py"),
                str(config.get("users", 50000)),
                str(config.get("avg_friends", 15)),
                size
            ])
            self.log.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç %s —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω", size)
            return True
        except subprocess.CalledProcessError as e:
            self.log.error("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", e)
            return False
    
    def copy_to_containers(self, size: str) -> bool:
        """–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã"""
        self.log.info("üì¶ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ %s –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã...", size)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
        users_file = self.base_path / size / "users.csv"
        friends_file = self.base_path / size / "friendships.csv"
        
        if not users_file.exists() or not friends_file.exists():
            self.log.error("‚ùå –§–∞–π–ª—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return False
        
        # –ö–æ–º–∞–Ω–¥—ã –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
        commands = [
            (["docker", "cp", str(users_file), f"{POSTGRES_CONTAINER}:/tmp/users.csv"], 
             "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ users -> Postgres"),
            (["docker", "exec", POSTGRES_CONTAINER, "chmod", "644", "/tmp/users.csv"],
             "–ü—Ä–∞–≤–∞ users.csv"),
            (["docker", "cp", str(friends_file), f"{POSTGRES_CONTAINER}:/tmp/friendships.csv"],
             "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ friendships -> Postgres"),
            (["docker", "exec", POSTGRES_CONTAINER, "chmod", "644", "/tmp/friendships.csv"],
             "–ü—Ä–∞–≤–∞ friendships.csv"),
            (["docker", "exec", NEO4J_CONTAINER, "mkdir", "-p", f"/var/lib/neo4j/import/{size}"],
             "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ Neo4j"),
            (["docker", "cp", str(users_file), f"{NEO4J_CONTAINER}:/var/lib/neo4j/import/{size}/users.csv"],
             "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ users -> Neo4j"),
            (["docker", "cp", str(friends_file), f"{NEO4J_CONTAINER}:/var/lib/neo4j/import/{size}/friendships.csv"],
             "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ friendships -> Neo4j")
        ]
        
        for cmd, desc in commands:
            if not self.retry_cmd(cmd):
                self.log.error("‚ùå –û—à–∏–±–∫–∞ —à–∞–≥–∞: %s", desc)
                return False
        
        self.log.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç %s —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã", size)
        return True
    
    def load_to_databases(self, size: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—ã"""
        self.log.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ %s –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –±–∞–∑—ã...", size)
        
        loader = self.scripts_path / "load_data.py"
        if not loader.exists():
            self.log.error("‚ùå –°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        try:
            self.run_cmd([sys.executable, str(loader), size])
            self.log.info("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –±–∞–∑—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            return True
        except subprocess.CalledProcessError:
            self.log.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
            return False
    
    def finalize_initialize_databases(self, infrastructure_config: str) -> bool:
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        self.log.info(f"üîß –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {infrastructure_config})...")
        try:
            self.run_cmd([sys.executable, str(self.scripts_path / "init_database.py"), "finalize", infrastructure_config])
            self.log.info("‚úÖ –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            return True
        except subprocess.CalledProcessError as e:
            self.log.error("‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏: %s", e.stderr.strip())
            return False
    
    def inspect_databases(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–∞—Ö"""
        self.log.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–Ω—ã—Ö...")
        try:
            self.run_cmd([sys.executable, str(self.scripts_path / "inspect_databases.py")])
            return True
        except subprocess.CalledProcessError as e:
            self.log.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: %s", e)
            return False
    
    def run_benchmarks(self, infrastructure_config: str, size: str, iteration: int, 
                       adaptive_runs: Dict[str, int]) -> Optional[Path]:
        """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        self.log.info(f"üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è {size} (–∏—Ç–µ—Ä–∞—Ü–∏—è {iteration}, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {infrastructure_config})...")
        
        runner = self.scripts_path / "benchmark_runner.py"
        if not runner.exists():
            self.log.error("‚ùå –°–∫—Ä–∏–ø—Ç –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø—Ä–æ–≥–æ–Ω–∞–º–∏
        config_file = self.results_path / f"config_{infrastructure_config}_{size}_{iteration}_{int(time.time())}.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(adaptive_runs, f, indent=2)
        
        # –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        result_file = self.results_path / f"results_{infrastructure_config}_{size}_{iteration}_{int(time.time())}.json"
        
        try:
            self.run_cmd([
                sys.executable, str(runner), infrastructure_config, size,
                "--config", str(config_file),
                "--output", str(result_file)
            ])
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
            config_file.unlink(missing_ok=True)
            
            if result_file.exists():
                self.log.info("‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ %s", result_file)
                return result_file
            else:
                self.log.error("‚ùå –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ —Å–æ–∑–¥–∞–Ω")
                return None
                
        except subprocess.CalledProcessError as e:
            self.log.error("‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: %s", e)
            return None
    
    def process_iteration(self, infrastructure_config: str, size: str, iteration: int, 
                         previous_size: str = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        start_time = time.time()
        result = {
            "infrastructure_config": infrastructure_config,
            "size": size,
            "iteration": iteration,
            "start_time": start_time,
            "status": "started",
            "adaptations": {},
            "errors": []
        }
        
        # –ü–æ–ª—É—á–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        adaptive_runs = self.query_manager.get_adaptive_config(size, previous_size)
        result["adaptations"]["query_runs"] = adaptive_runs
        
        # # –®–∞–≥ 1: –û—á–∏—Å—Ç–∫–∞
        # if not self.cleanup_databases(infrastructure_config):
        #     result["status"] = "cleanup_failed"
        #     result["errors"].append("–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö")
        #     return result
        
        # # –®–∞–≥ 2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        # if not self.initialize_databases(infrastructure_config):
        #     result["status"] = "init_failed"
        #     result["errors"].append("–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ö–µ–º")
        #     return result
        
        # # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        # if not self.generate_dataset(size):
        #     result["status"] = "generate_failed"
        #     result["errors"].append("–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        #     return result
        
        # # –®–∞–≥ 4: –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
        # if not self.copy_to_containers(size):
        #     result["status"] = "copy_failed"
        #     result["errors"].append("–û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã")
        #     return result
        
        # # –®–∞–≥ 5: –ó–∞–≥—Ä—É–∑–∫–∞
        # if not self.load_to_databases(size):
        #     result["status"] = "load_failed"
        #     result["errors"].append("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
        #     return result
        
        # # –®–∞–≥ 6: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
        # if not self.finalize_initialize_databases(infrastructure_config):
        #     result["status"] = "finalize_failed"
        #     result["errors"].append("–û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏")
        #     return result
        
        # –®–∞–≥ 7: –ü—Ä–æ–≤–µ—Ä–∫–∞
        if not self.inspect_databases():
            result["status"] = "inspect_failed"
            result["errors"].append("–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
            return result
        
        # –®–∞–≥ 8: –ë–µ–Ω—á–º–∞—Ä–∫–∏
        result_file = self.run_benchmarks(infrastructure_config, size, iteration, adaptive_runs)
        if not result_file:
            result["status"] = "benchmark_failed"
            result["errors"].append("–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤")
            return result
        
        # –ß—Ç–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                benchmark_data = json.load(f)
            
            efficiency_analysis = self.trend_analyzer.analyze_benchmark_result(benchmark_data)
            
            result.update({
                "status": "completed",
                "result_file": str(result_file),
                "efficiency_analysis": efficiency_analysis,
                "benchmark_data": benchmark_data,
                "end_time": time.time(),
                "duration": time.time() - start_time
            })
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            self.query_manager.update_from_results(size, benchmark_data)
            if efficiency_analysis:
                self.efficiency_history.append(efficiency_analysis)
            
            self.stats["successful_iterations"] += 1
            
        except Exception as e:
            result["status"] = "analysis_failed"
            result["errors"].append(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        
        self.stats["total_iterations"] += 1
        return result
    
    def analyze_current_trend(self) -> Tuple[bool, str, Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â–∏–π —Ç—Ä–µ–Ω–¥ –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ –æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏"""
        if len(self.efficiency_history) < 2:
            return False, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", {}
        
        trend_analysis = self.trend_analyzer.analyze_trends(self.efficiency_history)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        last_size = self.stats["sizes_completed"][-1] if self.stats["sizes_completed"] else "unknown"
        
        should_stop, reason = self.trend_analyzer.should_stop_based_on_trend(
            trend_analysis, last_size
        )
        
        return should_stop, reason, trend_analysis
    
    def run_adaptive_testing_for_config(self, infrastructure_config: str, target: str):
        """–ó–∞–ø—É—Å–∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        self.log.info("=" * 80)
        self.log.info(f"üöÄ –ó–ê–ü–£–°–ö –ê–î–ê–ü–¢–ò–í–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø: {infrastructure_config.upper()}")
        self.log.info("=" * 80)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if target == "all":
            sizes_to_process = ORDERED_SIZES
        elif target in ORDERED_SIZES:
            start_idx = ORDERED_SIZES.index(target)
            sizes_to_process = ORDERED_SIZES[start_idx:]
        else:
            self.log.error("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ü–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä: %s", target)
            return
        
        self.log.info("üìã –†–∞–∑–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: %s", " ‚Üí ".join(sizes_to_process))
        self.log.info("‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã: %s", infrastructure_config)
        
        previous_size = None
        stop_reason = None
        trend_history = []
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        for size_idx, size in enumerate(sizes_to_process):
            self.log.info("\n" + "=" * 80)
            self.log.info("üéØ –†–ê–ó–ú–ï–† %s (%d/%d)", size.upper(), size_idx + 1, len(sizes_to_process))
            self.log.info("=" * 80)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
            if size_idx > 0:
                should_stop, reason, trend_analysis = self.analyze_current_trend()
                trend_history.append(trend_analysis)
                
                if should_stop:
                    stop_reason = reason
                    self.log.info("üõë –ü–†–ò–ù–Ø–¢–û –†–ï–®–ï–ù–ò–ï –û–ë –û–°–¢–ê–ù–û–í–ö–ï: %s", reason)
                    break
            
            size_config = self.config.get(size, {})
            iterations = size_config.get("iterations", 1)
            
            self.log.info("üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: %d –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, %d —Å—Ä–µ–¥–Ω–∏—Ö –¥—Ä—É–∑–µ–π, %d –∏—Ç–µ—Ä–∞—Ü–∏–π",
                    size_config.get("users", 0),
                    size_config.get("avg_friends", 0),
                    iterations)
            
            size_start_time = time.time()
            size_results = []
            
            # –ó–∞–ø—É—Å–∫ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            for iteration in range(1, iterations + 1):
                self.log.info("-" * 60)
                self.log.info("üîÑ –ò–¢–ï–†–ê–¶–ò–Ø %d/%d –¥–ª—è %s", iteration, iterations, size)
                
                result = self.process_iteration(infrastructure_config, size, iteration, previous_size)
                size_results.append(result)
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏
                if result["status"] == "completed":
                    self.log.info("‚úÖ –ò—Ç–µ—Ä–∞—Ü–∏—è %d –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ %.2f —Å–µ–∫", 
                            iteration, result.get("duration", 0))
                    
                    if "efficiency_analysis" in result:
                        eff = result["efficiency_analysis"].get("summary", {})
                        avg_eff = eff.get("average_efficiency", 1.0)
                        winner = eff.get("overall_winner", "Unknown")
                        self.log.info("üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: %.2fx, –ü–æ–±–µ–¥–∏—Ç–µ–ª—å: %s", avg_eff, winner)
                else:
                    self.log.warning("‚ö†Ô∏è –ò—Ç–µ—Ä–∞—Ü–∏—è %d –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π: %s", 
                               iteration, result.get("status", "unknown"))
                    self.log.warning("   –û—à–∏–±–∫–∏: %s", result.get("errors", []))
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–º–µ—Ä–∞
            size_duration = time.time() - size_start_time
            self.save_size_results(infrastructure_config, size, size_results, size_duration)
            self.stats["sizes_completed"].append(size)
            
            previous_size = size
            
            # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É
            self.print_size_summary(size, size_results, size_duration)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
        self.print_final_summary(infrastructure_config, stop_reason, trend_history)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        self.save_full_report(infrastructure_config, stop_reason)
    
    def save_size_results(self, infrastructure_config: str, size: str, results: List[Dict[str, Any]], duration: float):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞"""
        summary = {
            "infrastructure_config": infrastructure_config,
            "size": size,
            "config": self.config.get(size, {}),
            "iterations": len(results),
            "successful_iterations": sum(1 for r in results if r["status"] == "completed"),
            "duration": duration,
            "results": results,
            "timestamp": datetime.now().isoformat()
        }
        
        summary_file = self.results_path / f"{infrastructure_config}_{size}_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.log.info("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: %s", summary_file)
    
    def print_size_summary(self, size: str, results: List[Dict[str, Any]], duration: float):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É"""
        successful = sum(1 for r in results if r["status"] == "completed")
        total = len(results)
        
        if successful == 0:
            self.log.warning("‚ùå –†–∞–∑–º–µ—Ä %s: 0 —É—Å–ø–µ—à–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π –∏–∑ %d", size, total)
            return
        
        # –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        efficiencies = []
        for result in results:
            if result["status"] == "completed" and "efficiency_analysis" in result:
                eff = result["efficiency_analysis"].get("summary", {}).get("average_efficiency", 1.0)
                efficiencies.append(eff)
        
        if efficiencies:
            avg_eff = statistics.mean(efficiencies)
            median_eff = statistics.median(efficiencies)
            min_eff = min(efficiencies)
            max_eff = max(efficiencies)
            
            self.log.info("üìä –°–í–û–î–ö–ê –ü–û –†–ê–ó–ú–ï–†–£ %s:", size.upper())
            self.log.info("   –ò—Ç–µ—Ä–∞—Ü–∏–π: %d/%d —É—Å–ø–µ—à–Ω–æ", successful, total)
            self.log.info("   –í—Ä–µ–º—è: %.2f –º–∏–Ω—É—Ç", duration / 60)
            self.log.info("   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Neo4j/PostgreSQL:")
            self.log.info("     ‚Ä¢ –°—Ä–µ–¥–Ω—è—è: %.2fx", avg_eff)
            self.log.info("     ‚Ä¢ –ú–µ–¥–∏–∞–Ω–Ω–∞—è: %.2fx", median_eff)
            self.log.info("     ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω: %.2fx - %.2fx", min_eff, max_eff)
            
            if avg_eff > 1.0:
                self.log.info("     üìà Neo4j –±—ã—Å—Ç—Ä–µ–µ –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ %.1f%%", (avg_eff - 1) * 100)
            else:
                self.log.info("     üìâ PostgreSQL –±—ã—Å—Ç—Ä–µ–µ –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ %.1f%%", (1 - avg_eff) * 100)
    
    def print_final_summary(self, infrastructure_config: str, stop_reason: Optional[str], trend_history: List[Dict[str, Any]]):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–≤–æ–¥–∫–∏"""
        self.log.info("\n" + "=" * 80)
        self.log.info(f"üèÅ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û: {infrastructure_config.upper()}")
        self.log.info("=" * 80)
        
        self.log.info("üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        self.log.info("   –í—Å–µ–≥–æ –∏—Ç–µ—Ä–∞—Ü–∏–π: %d", self.stats["total_iterations"])
        self.log.info("   –£—Å–ø–µ—à–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π: %d", self.stats["successful_iterations"])
        self.log.info("   –ü—Ä–æ–≤–∞–ª–µ–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π: %d", self.stats["failed_iterations"])
        self.log.info("   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–∑–º–µ—Ä–æ–≤: %d", len(self.stats["sizes_completed"]))
        self.log.info("   –ü—Ä–∏–º–µ–Ω–µ–Ω–æ –∞–¥–∞–ø—Ç–∞—Ü–∏–π: %d", self.stats.get("adaptations_applied", 0))
        
        if self.efficiency_history:
            # –ê–Ω–∞–ª–∏–∑ –∏—Ç–æ–≥–æ–≤–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            final_eff = self.efficiency_history[-1].get("summary", {}).get("average_efficiency", 1.0)
            overall_winner = self.efficiency_history[-1].get("summary", {}).get("overall_winner", "Unknown")
            
            self.log.info("üìä –ò–¢–û–ì–û–í–ê–Ø –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨:")
            self.log.info("   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: %.2fx", final_eff)
            self.log.info("   –û–±—â–∏–π –ø–æ–±–µ–¥–∏—Ç–µ–ª—å: %s", overall_winner)
            
            if final_eff > 1.0:
                self.log.info("   üéâ Neo4j –ø–æ–∫–∞–∑–∞–ª –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ %.1f%%", (final_eff - 1) * 100)
            else:
                self.log.info("   ‚ö° PostgreSQL –ø–æ–∫–∞–∑–∞–ª –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ %.1f%%", (1 - final_eff) * 100)
        
        if stop_reason:
            self.log.info("üõë –ü–†–ò–ß–ò–ù–ê –û–°–¢–ê–ù–û–í–ö–ò:")
            self.log.info("   %s", stop_reason)
        
        if trend_history:
            self.log.info("üìà –ê–ù–ê–õ–ò–ó –¢–†–ï–ù–î–û–í:")
            for i, trend in enumerate(trend_history):
                if trend.get("has_trend"):
                    self.log.info("   –†–∞–∑–º–µ—Ä %d: %s (–Ω–∞–∫–ª–æ–Ω: %.3f)", 
                            i + 1, trend.get("trend", "unknown"), trend.get("slope", 0))
    
    def save_full_report(self, infrastructure_config: str, stop_reason: Optional[str]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "infrastructure_config": infrastructure_config,
                "stop_reason": stop_reason,
                "total_duration": self.stats.get("total_time", 0)
            },
            "statistics": self.stats,
            "efficiency_history": self.efficiency_history,
            "testing_log": self.testing_log,
            "adaptations": self.query_manager.get_test_recommendations(),
            "config_used": self.config
        }
        
        report_file = self.results_path / f"{infrastructure_config}_full_report_{int(time.time())}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.log.info("üíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: %s", report_file)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python adaptive_testing.py [size / all] [--config poor|medium|rich|all] [--dry-run]")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python adaptive_testing.py small --config medium")
        print("  python adaptive_testing.py all --config rich")
        print("  python adaptive_testing.py all --config all    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        print("  python adaptive_testing.py super-tiny --dry-run")
        print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã:", " ‚Üí ".join(ORDERED_SIZES))
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤:", ", ".join(CONFIGS + ["all"]))
        return
    
    target = sys.argv[1]
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    config_arg = "all"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    dry_run = False
    
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == "--config" and i + 1 < len(sys.argv):
            config_arg = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == "--dry-run":
            dry_run = True
            i += 1
        else:
            i += 1
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å
    if config_arg == "all":
        configs_to_test = CONFIGS
    elif config_arg in CONFIGS:
        configs_to_test = [config_arg]
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {config_arg}")
        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {', '.join(CONFIGS + ['all'])}")
        return
    
    print("=" * 80)
    print("üöÄ –ó–ê–ü–£–°–ö –ú–ù–û–ì–û–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 80)
    print(f"üìã –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {target}")
    print(f"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {', '.join(configs_to_test)}")
    print(f"üëÅÔ∏è  –†–µ–∂–∏–º dry-run: {'–î–∞' if dry_run else '–ù–µ—Ç'}")
    print("=" * 80)
    
    overall_start_time = time.time()
    all_results = {}
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    for config_idx, config_name in enumerate(configs_to_test):
        config_start_time = time.time()
        
        print(f"\n\nüìä –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø {config_name.upper()} ({config_idx + 1}/{len(configs_to_test)})")
        print("-" * 60)
        
        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —ç—Ç–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        manager = AdaptiveTestingManager(config_name=config_name, dry_run=dry_run)
        
        try:
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç—Ç–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            manager.run_adaptive_testing_for_config(config_name, target)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_results[config_name] = {
                "stats": manager.stats,
                "efficiency_history": manager.efficiency_history,
                "sizes_completed": manager.stats["sizes_completed"]
            }
            
            config_duration = time.time() - config_start_time
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_name}: {config_duration:.2f} —Å–µ–∫")
            
        except KeyboardInterrupt:
            print(f"‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_name} –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_name}: {e}")
            import traceback
            traceback.print_exc()
    
    # –û–±—â–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ –≤—Å–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º
    overall_duration = time.time() - overall_start_time
    print("\n" + "=" * 80)
    print("üèÅ –ú–ù–û–ì–û–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 80)
    
    print("üìä –û–ë–©–ê–Ø –°–í–û–î–ö–ê –ü–û –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø–ú:")
    for config_name, results in all_results.items():
        stats = results.get("stats", {})
        print(f"\n  üìà {config_name.upper()}:")
        print(f"     –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–∑–º–µ—Ä–æ–≤: {len(stats.get('sizes_completed', []))}")
        print(f"     –£—Å–ø–µ—à–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π: {stats.get('successful_iterations', 0)}")
        print(f"     –ü—Ä–æ–≤–∞–ª–µ–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π: {stats.get('failed_iterations', 0)}")
        
        if results.get("efficiency_history"):
            last_eff = results["efficiency_history"][-1].get("summary", {}).get("average_efficiency", 1.0)
            winner = results["efficiency_history"][-1].get("summary", {}).get("overall_winner", "Unknown")
            print(f"     –ò—Ç–æ–≥–æ–≤–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {last_eff:.2f}x")
            print(f"     –ü–æ–±–µ–¥–∏—Ç–µ–ª—å: {winner}")
    
    print(f"\n‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {overall_duration:.2f} —Å–µ–∫ ({overall_duration/60:.2f} –º–∏–Ω)")
    print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–∞—Ö:")
    for config_name in configs_to_test:
        config_dir = RESULTS_DIR / config_name
        if config_dir.exists():
            print(f"   ‚Ä¢ {config_name}: {config_dir}")
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    create_comparative_report(all_results, configs_to_test, overall_duration)

def create_comparative_report(all_results: Dict[str, Any], configs_tested: List[str], total_duration: float):
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –≤—Å–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º"""
    comparative_data = {
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "configs_tested": configs_tested,
            "total_duration": total_duration
        },
        "config_comparison": {},
        "summary": {
            "best_config_per_size": {},
            "overall_best_config": None,
            "performance_differences": {}
        }
    }
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    for config_name, results in all_results.items():
        stats = results.get("stats", {})
        efficiency_history = results.get("efficiency_history", [])
        
        comparative_data["config_comparison"][config_name] = {
            "sizes_completed": stats.get("sizes_completed", []),
            "successful_iterations": stats.get("successful_iterations", 0),
            "failed_iterations": stats.get("failed_iterations", 0),
            "final_efficiency": efficiency_history[-1].get("summary", {}).get("average_efficiency", 1.0) if efficiency_history else 1.0,
            "final_winner": efficiency_history[-1].get("summary", {}).get("overall_winner", "Unknown") if efficiency_history else "Unknown"
        }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    comp_report_file = RESULTS_DIR / f"comparative_report_{int(time.time())}.json"
    with open(comp_report_file, 'w', encoding='utf-8') as f:
        json.dump(comparative_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {comp_report_file}")
    
    # –ü—Ä–æ—Å—Ç–æ–π –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
    print("\nüìà –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó:")
    print("-" * 60)
    
    for config_name, data in comparative_data["config_comparison"].items():
        print(f"{config_name.upper():10} | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {data['final_efficiency']:6.2f}x | "
              f"–ü–æ–±–µ–¥–∏—Ç–µ–ª—å: {data['final_winner']:15} | –†–∞–∑–º–µ—Ä–æ–≤: {len(data['sizes_completed'])}")

if __name__ == "__main__":
    main()
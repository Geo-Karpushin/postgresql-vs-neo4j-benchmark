#!/usr/bin/env python3
"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤.
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞.
–° –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π –ø—Ä–∏ —è–≤–Ω–æ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ Neo4j.
"""

import subprocess
import sys
import time
import json
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime
import statistics

DATA_DIR = Path("generated")
SCRIPTS_DIR = Path("scripts")
RESULTS_DIR = Path("results")
POSTGRES_CONTAINER = "database-benchmark-postgres-1"
NEO4J_CONTAINER = "database-benchmark-neo4j-1"
DOCKER_RETRIES = 4
DOCKER_BACKOFF = 2

CONFIGS = [
    "test",
    "poor",
    "medium",
    "rich"
]

# –£–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É
ORDERED_SIZES = [
    "super-tiny",
    "tiny", 
    "very-small",
    "small",
    "medium",
    "large",
    "x-large",
    "xx-large"
]

DATASETS_CONFIG = {
    "super-tiny": {
        "users": 5_000,
        "avg_friends": 25,
        "iterations": 5,
        "query_runs": {
            "simple_friends": 150,
            "friends_of_friends": 300,
            "mutual_friends": 150,
            "friend_recommendations": 50,
            "shortest_path": 10,
            "cohort_analysis": 10,
            "social_cities": 8,
            "age_gap_analysis": 8,
            "network_growth": 3,
            "age_clustering": 3
        }
    },
    "tiny": {
        "users": 10_000,
        "avg_friends": 22,
        "iterations": 5,
        "query_runs": {
            "simple_friends": 120,
            "friends_of_friends": 250,
            "mutual_friends": 120,
            "friend_recommendations": 40,
            "shortest_path": 8,
            "cohort_analysis": 8,
            "social_cities": 6,
            "age_gap_analysis": 6,
            "network_growth": 3,
            "age_clustering": 3
        }
    },
    "very-small": {
        "users": 20_000,
        "avg_friends": 20,
        "iterations": 5,
        "query_runs": {
            "simple_friends": 100,
            "friends_of_friends": 200,
            "mutual_friends": 100,
            "friend_recommendations": 30,
            "shortest_path": 6,
            "cohort_analysis": 6,
            "social_cities": 5,
            "age_gap_analysis": 5,
            "network_growth": 3,
            "age_clustering": 3
        }
    },
    "small": {
        "users": 50_000,
        "avg_friends": 20,
        "iterations": 5,
        "query_runs": {
            "simple_friends": 50,
            "friends_of_friends": 400,
            "mutual_friends": 50,
            "friend_recommendations": 20,
            "shortest_path": 5,
            "cohort_analysis": 5,
            "social_cities": 4,
            "age_gap_analysis": 4,
            "network_growth": 3,
            "age_clustering": 3
        }
    },
    "medium": {
        "users": 500_000,
        "avg_friends": 18,
        "iterations": 3,
        "query_runs": {
            "simple_friends": 40,
            "friends_of_friends": 100,
            "mutual_friends": 40,
            "friend_recommendations": 20,
            "shortest_path": 5,
            "cohort_analysis": 4,
            "social_cities": 3,
            "age_gap_analysis": 3,
            "network_growth": 3,
            "age_clustering": 3
        }
    },
    "large": {
        "users": 2_000_000,
        "avg_friends": 15,
        "iterations": 1,
        "query_runs": {
            "simple_friends": 30,
            "friends_of_friends": 80,
            "mutual_friends": 30,
            "friend_recommendations": 15,
            "shortest_path": 3,
            "cohort_analysis": 3,
            "social_cities": 3,
            "age_gap_analysis": 3,
            "network_growth": 3,
            "age_clustering": 3
        }
    },
    "x-large": {
        "users": 5_000_000,
        "avg_friends": 12,
        "iterations": 1,
        "query_runs": {
            "simple_friends": 20,
            "friends_of_friends": 50,
            "mutual_friends": 20,
            "friend_recommendations": 10,
            "shortest_path": 3,
            "cohort_analysis": 3,
            "social_cities": 3,
            "age_gap_analysis": 3,
            "network_growth": 3,
            "age_clustering": 3
        }
    },
    "xx-large": {
        "users": 10_000_000,
        "avg_friends": 10,
        "iterations": 1,
        "query_runs": {
            "simple_friends": 10,
            "friends_of_friends": 30,
            "mutual_friends": 10,
            "friend_recommendations": 5,
            "shortest_path": 3,
            "cohort_analysis": 3,
            "social_cities": 3,
            "age_gap_analysis": 3,
            "network_growth": 3,
            "age_clustering": 3
        }
    }
}

for config_name, config in DATASETS_CONFIG.items():
    users = config["users"]
    avg_friends = config["avg_friends"]
    friendships_count = int(users * avg_friends)
    config["expected_friendships"] = friendships_count
    
    if users <= 50_000:
        config["estimated_time_minutes"] = 5
    elif users <= 500_000:
        config["estimated_time_minutes"] = 15
    elif users <= 2_000_000:
        config["estimated_time_minutes"] = 30
    elif users <= 5_000_000:
        config["estimated_time_minutes"] = 60
    else:
        config["estimated_time_minutes"] = 120

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
log = logging.getLogger("dataset_manager")

def run_cmd(cmd: List[str], capture: bool = True, check: bool = True) -> subprocess.CompletedProcess:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥ (–±–µ–∑ shell=True)."""
    return subprocess.run(cmd, text=True, capture_output=capture, check=check)

def retry_cmd(cmd: List[str], retries: int = DOCKER_RETRIES, backoff: int = DOCKER_BACKOFF) -> bool:
    """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–º–∞–Ω–¥—É —Å retry/backoff. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –ø—Ä–∏ —É—Å–ø–µ—Ö–µ, False –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ."""
    attempt = 0
    delay = backoff
    while attempt < retries:
        try:
            run_cmd(cmd)
            return True
        except subprocess.CalledProcessError as e:
            attempt += 1
            log.warning("–ö–æ–º–∞–Ω–¥–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å (–ø–æ–ø—ã—Ç–∫–∞ %d/%d): %s ‚Äî %s", attempt, retries, " ".join(cmd), e.stderr.strip()[:200])
            if attempt < retries:
                time.sleep(delay)
                delay *= 2
    return False

class EfficiencyAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Neo4j"""
    
    @staticmethod
    def analyze_benchmark_result(result_file: Path) -> Optional[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            efficiency = data.get("efficiency", {})
            if not efficiency:
                return None
            
            summary = efficiency.get("_summary", {})
            if not summary:
                return None
            
            return {
                "average_efficiency": summary.get("average_efficiency", 1.0),
                "median_efficiency": summary.get("median_efficiency", 1.0),
                "neo4j_wins_count": summary.get("neo4j_wins_count", 0),
                "postgres_wins_count": summary.get("postgres_wins_count", 0),
                "total_comparisons": summary.get("total_comparisons", 0),
                "overall_winner": summary.get("overall_winner", "None"),
                "performance_advantage": summary.get("performance_advantage", "0%")
            }
        except Exception as e:
            log.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {result_file}: {e}")
            return None
    
    @staticmethod
    def is_neo4j_clearly_faster(efficiency_data: Dict[str, Any]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∏–º–µ–µ—Ç –ª–∏ Neo4j —è–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ.
        –£—Å–ª–æ–≤–∏—è:
        1. Neo4j –≤—ã–∏–≥—Ä–∞–ª –±–æ–ª—å—à–µ –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ–º PostgreSQL
        2. –°—Ä–µ–¥–Ω–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ > 1.5
        3. –ú–µ–¥–∏–∞–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ > 1.2
        """
        if not efficiency_data:
            return False
        
        neo_wins = efficiency_data.get("neo4j_wins_count", 0)
        pg_wins = efficiency_data.get("postgres_wins_count", 0)
        avg_eff = efficiency_data.get("average_efficiency", 1.0)
        median_eff = efficiency_data.get("median_efficiency", 1.0)
        
        # Neo4j –≤—ã–∏–≥—Ä–∞–ª –±–æ–ª—å—à–µ –∑–∞–ø—Ä–æ—Å–æ–≤
        has_more_wins = neo_wins > pg_wins
        
        # –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—É
        has_high_avg_efficiency = avg_eff > 1.5
        
        # –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø–æ –º–µ–¥–∏–∞–Ω–Ω–æ–º—É –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—É
        has_high_median_efficiency = median_eff > 1.2
        
        return has_more_wins and has_high_avg_efficiency and has_high_median_efficiency
    
    @staticmethod
    def calculate_size_efficiency(size_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—É–º–º–∞—Ä–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        if not size_results:
            return {"average_efficiency": 1.0, "neo4j_clearly_faster": False}
        
        avg_efficiencies = [r.get("average_efficiency", 1.0) for r in size_results]
        neo_clearly_faster_flags = [EfficiencyAnalyzer.is_neo4j_clearly_faster(r) for r in size_results]
        
        return {
            "average_efficiency": statistics.mean(avg_efficiencies),
            "median_efficiency": statistics.median(avg_efficiencies),
            "neo4j_clearly_faster_percentage": (sum(neo_clearly_faster_flags) / len(neo_clearly_faster_flags)) * 100,
            "is_neo4j_consistently_faster": all(neo_clearly_faster_flags),
            "iterations": len(size_results)
        }

class DatasetManager:
    def __init__(self, dry_run: bool = False):
        self.base_path = DATA_DIR
        self.scripts_path = SCRIPTS_DIR
        self.results_path = RESULTS_DIR
        self.dry_run = dry_run
        self.results_path.mkdir(parents=True, exist_ok=True)
        self.config = DATASETS_CONFIG
        self.efficiency_analyzer = EfficiencyAnalyzer()
        self.sizes_history = []  # –ò—Å—Ç–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤
        
    def _ensure_dataset_files(self, size: str) -> bool:
        users = self.base_path / size / "users.csv"
        friendships = self.base_path / size / "friendships.csv"
        if not users.exists():
            log.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª: %s", users)
            return False
        if not friendships.exists():
            log.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª: %s", friendships)
            return False
        return True

    def initialize_databases(self) -> bool:
        log.info("üóÉÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
        try:
            subprocess.run([sys.executable, str(self.scripts_path / "init_database.py"), "init"], check=True)
            log.info("‚úÖ –°—Ö–µ–º—ã –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: %s", e.stderr.strip())
            return False
        
    def cleanup_databases(self, config) -> bool:
        log.info("üßπ –û—á–∏—Å—Ç–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
        try:
            subprocess.run(
                [sys.executable, str(self.scripts_path / "cleanup_databases.py"),
                "--config", str(config)],
                check=True
            )
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: %s", e)
            return False
    
    def inspect_databases(self) -> bool:
        log.info("üìù –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–Ω—ã—Ö...")
        try:
            subprocess.run(
                [sys.executable, str(self.scripts_path / "inspect_databases.py")],
                check=True
            )
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: %s", e)
            return False

    def generate_dataset(self, size: str) -> bool:
        log.info("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ %s...", size)
        try:
            config = self.config.get(size, {})
            subprocess.run(
                [sys.executable, str(self.scripts_path / "data_generator.py"), 
                str(config.get("users", 50000)), 
                str(config.get("avg_friends", 15)), 
                size],
                check=True
            )
            log.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç %s —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω", size)
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: %s", e)
            return False

    def copy_to_containers(self, size: str) -> bool:
        log.info("üì¶ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ %s –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã...", size)
        if not self._ensure_dataset_files(size):
            return False

        users_host = str(self.base_path / size / "users.csv")
        friends_host = str(self.base_path / size / "friendships.csv")

        cp_pg_users = ["docker", "cp", users_host, f"{POSTGRES_CONTAINER}:/tmp/users.csv"]
        cp_pg_friends = ["docker", "cp", friends_host, f"{POSTGRES_CONTAINER}:/tmp/friendships.csv"]
        cmd_chmod_users = ["docker", "exec", POSTGRES_CONTAINER, "chmod", "644", "/tmp/users.csv"]
        cmd_chmod_friends = ["docker", "exec", POSTGRES_CONTAINER, "chmod", "644", "/tmp/friendships.csv"]

        neo4j_dir = f"/var/lib/neo4j/import/{size}"
        mkdir_neo = ["docker", "exec", NEO4J_CONTAINER, "mkdir", "-p", neo4j_dir]
        cp_neo_users = ["docker", "cp", users_host, f"{NEO4J_CONTAINER}:{neo4j_dir}/users.csv"]
        cp_neo_friends = ["docker", "cp", friends_host, f"{NEO4J_CONTAINER}:{neo4j_dir}/friendships.csv"]

        steps = [
            (mkdir_neo, "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ Neo4j"),
            (cp_pg_users, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ users -> Postgres"),
            (cmd_chmod_users, "–í—ã–¥–∞—á–∞ –ø—Ä–∞–≤ users.csv"),
            (cp_pg_friends, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ friendships -> Postgres"),
            (cmd_chmod_friends, "–í—ã–¥–∞—á–∞ –ø—Ä–∞–≤ friendships.csv"),
            (cp_neo_users, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ users -> Neo4j"),
            (cp_neo_friends, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ friendships -> Neo4j"),
        ]

        for cmd, desc in steps:
            log.info("  ‚Ä¢ %s: %s", desc, " ".join(cmd) if self.dry_run else "")
            if self.dry_run:
                continue
            ok = retry_cmd(cmd)
            if not ok:
                log.error("  ‚ùå –û—à–∏–±–∫–∞ —à–∞–≥–∞: %s", desc)
                return False

        log.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç %s —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã", size)
        return True

    def load_to_databases(self, size: str) -> bool:
        log.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ %s –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –±–∞–∑—ã...", size)

        loader = self.scripts_path / "load_data.py"
        if not loader.exists():
            log.error("–°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", loader)
            return False

        try:
            subprocess.run(
                [
                    sys.executable,
                    str(loader),
                    size,
                ],
                check=True
            )
            log.info("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –±–∞–∑—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True

        except subprocess.CalledProcessError:
            log.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
            return False

    def finalize_initialize_databases(self) -> bool:
        log.info("üóÉÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
        try:
            subprocess.run([sys.executable, str(self.scripts_path / "init_database.py"), "finalize"], check=True)
            log.info("‚úÖ –°—Ö–µ–º—ã –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: %s", e.stderr.strip())
            return False

    def run_benchmarks(self, setup_config:str, size: str, iteration: int) -> Optional[Path]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        log.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è %s (–∏—Ç–µ—Ä–∞—Ü–∏—è %d)...", size, iteration)

        runner = self.scripts_path / "benchmark_runner.py"
        if not runner.exists():
            log.error("–°–∫—Ä–∏–ø—Ç –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", runner)
            return None

        try:
            config = self.config.get(size, {})
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª –¥–ª—è query_runs
            config_file = self.results_path / f"benchmark_config_{setup_config}_{size}_{iteration}.json"

            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config.get("query_runs", {}), f, indent=2)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            result_filename = f"benchmark_results_{size}_{iteration}_{int(time.time())}.json"
            result_file = self.results_path / result_filename
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –ø—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            subprocess.run(
                [
                    sys.executable, str(runner), setup_config, size,
                    "--config", str(config_file),
                    "--output", str(result_file)
                ],
                check=True
            )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª
            config_file.unlink(missing_ok=True)
            
            if result_file.exists():
                log.info("‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è %s –∑–∞–≤–µ—Ä—à–µ–Ω—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ %s", size, result_file)
                return result_file
            else:
                log.error("‚ùå –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ —Å–æ–∑–¥–∞–Ω: %s", result_file)
                return None

        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: %s", e)
            return None

    def process_iteration(self, config: str, size: str, iteration: int) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
        result = {
            "size": size,
            "iteration": iteration,
            "timestamps": {},
            "durations": {},
            "status": "unknown",
            "config": self.config.get(size, {}),
            "start_time": time.time()
        }

        # 1. –û—á–∏—Å—Ç–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
        t0 = time.time()
        ok = self.cleanup_databases(config)
        result["timestamps"]["cleanup_start"] = t0
        result["durations"]["cleanup"] = time.time() - t0
        if not ok:
            result["status"] = "cleanup_failed"
            result["end_time"] = time.time()
            return result
        
        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º
        t1 = time.time()
        ok = self.initialize_databases()
        result["timestamps"]["initialize_start"] = t1
        result["durations"]["initialize"] = time.time() - t1
        if not ok:
            result["status"] = "initialize_failed"
            result["end_time"] = time.time()
            return result

        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        t2 = time.time()
        ok = self.generate_dataset(size)
        result["timestamps"]["generate_start"] = t2
        result["durations"]["generate"] = time.time() - t2
        if not ok:
            result["status"] = "generate_failed"
            result["end_time"] = time.time()
            return result

        # 4. –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
        t3 = time.time()
        ok = self.copy_to_containers(size)
        result["timestamps"]["copy_start"] = t3
        result["durations"]["copy"] = time.time() - t3
        if not ok:
            result["status"] = "copy_failed"
            result["end_time"] = time.time()
            return result

        # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        t4 = time.time()
        ok = self.load_to_databases(size)
        result["timestamps"]["load_start"] = t4
        result["durations"]["load"] = time.time() - t4
        if not ok:
            result["status"] = "load_failed"
            result["end_time"] = time.time()
            return result
        
        # 6. –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
        t5 = time.time()
        ok = self.finalize_initialize_databases()
        result["timestamps"]["finalize_initialize_start"] = t5
        result["durations"]["finalize_initialize"] = time.time() - t5
        if not ok:
            result["status"] = "finalize_initialize_failed"
            result["end_time"] = time.time()
            return result

        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        ok = self.inspect_databases()
        if not ok:
            result["status"] = "inspection_failed"
            result["end_time"] = time.time()
            return result

        # 7. –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
        t6 = time.time()
        result_file = self.run_benchmarks(config, size, iteration)
        result["timestamps"]["benchmark_start"] = t6
        result["durations"]["benchmark"] = time.time() - t6
        if result_file is None:
            result["status"] = "bench_failed"
            result["end_time"] = time.time()
            return result
        
        result["benchmark_result_file"] = str(result_file)
        
        # 8. –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        efficiency_data = self.efficiency_analyzer.analyze_benchmark_result(result_file)
        if efficiency_data:
            result["efficiency_analysis"] = efficiency_data
            result["neo4j_clearly_faster"] = self.efficiency_analyzer.is_neo4j_clearly_faster(efficiency_data)

        result["status"] = "ok"
        result["end_time"] = time.time()
        result["total_time"] = result["end_time"] - result["start_time"]
        return result

    def should_stop_testing(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.
        –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º, –µ—Å–ª–∏ Neo4j –ø–æ–∫–∞–∑–∞–ª —è–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ
        –Ω–∞ –¥–≤—É—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞.
        """
        if len(self.sizes_history) < 2:
            return False
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–∞ —Ä–∞–∑–º–µ—Ä–∞
        recent_sizes = self.sizes_history[-2:]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∏ –ª–∏ –æ–±–∞ —Ä–∞–∑–º–µ—Ä–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
        for size_info in recent_sizes:
            if not size_info.get("completed", False):
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–∫–∞–∑–∞–ª –ª–∏ Neo4j —è–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–∞ –æ–±–æ–∏—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö
        neo4j_faster_count = 0
        for size_info in recent_sizes:
            if size_info.get("neo4j_consistently_faster", False):
                neo4j_faster_count += 1
        
        if neo4j_faster_count >= 2:
            log.info("üö® Neo4j –ø–æ–∫–∞–∑–∞–ª —è–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–∞ –¥–≤—É—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞!")
            log.info("   –†–∞–∑–º–µ—Ä—ã: %s –∏ %s", 
                    recent_sizes[0]["size"], recent_sizes[1]["size"])
            return True
        
        return False

def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python dataset_manager.py [size / all] [--dry-run]")
        print("–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python dataset_manager.py small")
        print("  python dataset_manager.py all")
        print("  python dataset_manager.py all --dry-run")
        return

    target = sys.argv[1]
    dry = "--dry-run" in sys.argv

    manager = DatasetManager(dry_run=dry)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ —Ä–∞–∑–º–µ—Ä—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å
    if target == "all":
        sizes_to_process = ORDERED_SIZES
        log.info("üéØ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
        log.info("üìä –ü–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏: %s", " ‚Üí ".join(sizes_to_process))
    elif target in ORDERED_SIZES:
        sizes_to_process = ORDERED_SIZES[ORDERED_SIZES.index(target):]
        log.info("üéØ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Ä–∞–∑–º–µ—Ä–∞: %s", target)
        log.info("üìä –ë—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: %s", " ‚Üí ".join(sizes_to_process))
    else:
        log.error("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: %s", target)
        log.error("   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã: %s", ", ".join(ORDERED_SIZES))
        return

    # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    overall_results_file = manager.results_path / f"overall_results_{int(time.time())}.json"
    overall_results = {
        "start_time": datetime.now().isoformat(),
        "sizes_processed": [],
        "stopped_early": False,
        "stop_reason": None
    }

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–∞–∑–º–µ—Ä
    for config in CONFIGS:
        for size_idx, size in enumerate(sizes_to_process):
            log.info("=" * 80)
            log.info("üéØ –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–¢–ê–°–ï–¢–ê: %s (%d/%d)", 
                    size.upper(), size_idx + 1, len(sizes_to_process))
            log.info("üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: %s –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, %s —Å—Ä–µ–¥–Ω–∏—Ö –¥—Ä—É–∑–µ–π, %s –∏—Ç–µ—Ä–∞—Ü–∏–π", 
                    manager.config.get(size, {}).get("users", "N/A"),
                    manager.config.get(size, {}).get("avg_friends", "N/A"),
                    manager.config.get(size, {}).get("iterations", "N/A"))
            
            size_results = []
            efficiency_results = []
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            iterations = manager.config.get(size, {}).get("iterations", 1)
            for iteration in range(1, iterations + 1):
                log.info("üîÑ –ò–¢–ï–†–ê–¶–ò–Ø %i/%i –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞ %s", iteration, iterations, size)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏—é
                iteration_result = manager.process_iteration(config, size, iteration)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏—Ç–µ—Ä–∞—Ü–∏–∏
                iteration_file = manager.results_path / f"{size}_iteration_{iteration}.json"
                with open(iteration_file, "w", encoding="utf-8") as f:
                    json.dump(iteration_result, f, ensure_ascii=False, indent=2)
                log.info("üìù –†–µ–∑—É–ª—å—Ç–∞—Ç –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: %s", iteration_file)
                
                if iteration_result["status"] != "ok":
                    log.warning("‚ö†Ô∏è –ò—Ç–µ—Ä–∞—Ü–∏—è %i –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å —Å—Ç–∞—Ç—É—Å–æ–º: %s", 
                            iteration, iteration_result["status"])
                    continue
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                if "efficiency_analysis" in iteration_result:
                    efficiency_results.append(iteration_result["efficiency_analysis"])
                    neo4j_faster = iteration_result.get("neo4j_clearly_faster", False)
                    
                    if neo4j_faster:
                        log.info("‚úÖ Neo4j –ø–æ–∫–∞–∑–∞–ª —è–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ %d", iteration)
                    else:
                        log.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Ç–µ—Ä–∞—Ü–∏–∏ %d: Neo4j %s", iteration,
                                "–±—ã—Å—Ç—Ä–µ–µ" if neo4j_faster else "–Ω–µ –ø–æ–∫–∞–∑–∞–ª —è–≤–Ω–æ–≥–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞")
                
                size_results.append(iteration_result)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—â—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞
            size_efficiency = manager.efficiency_analyzer.calculate_size_efficiency(efficiency_results)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
            size_info = {
                "size": size,
                "completed": len(size_results) > 0,
                "successful_iterations": len([r for r in size_results if r["status"] == "ok"]),
                "total_iterations": iterations,
                "neo4j_consistently_faster": size_efficiency.get("is_neo4j_consistently_faster", False),
                "average_efficiency": size_efficiency.get("average_efficiency", 1.0),
                "neo4j_faster_percentage": size_efficiency.get("neo4j_clearly_faster_percentage", 0.0)
            }
            manager.sizes_history.append(size_info)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É –ø–æ —Ä–∞–∑–º–µ—Ä—É
            size_summary = {
                "size": size,
                "config": manager.config.get(size, {}),
                "efficiency_summary": size_efficiency,
                "iterations_completed": len(size_results),
                "successful_iterations": len([r for r in size_results if r["status"] == "ok"]),
                "timestamp": datetime.now().isoformat()
            }
            
            summary_file = manager.results_path / f"{size}_summary.json"
            with open(summary_file, "w", encoding="utf-8") as f:
                json.dump(size_summary, f, ensure_ascii=False, indent=2)
            
            log.info("üìä –°–í–û–î–ö–ê –ü–û –†–ê–ó–ú–ï–†–£ %s:", size.upper())
            log.info("   –°—Ä–µ–¥–Ω–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: %.2fx", size_efficiency.get("average_efficiency", 1.0))
            log.info("   Neo4j –ø–æ–∫–∞–∑–∞–ª –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –≤ %.1f%% –∏—Ç–µ—Ä–∞—Ü–∏–π", 
                    size_efficiency.get("neo4j_clearly_faster_percentage", 0.0))
            log.info("   Neo4j —Å—Ç–∞–±–∏–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ: %s", 
                    "–î–ê" if size_efficiency.get("is_neo4j_consistently_faster", False) else "–ù–ï–¢")
            
            overall_results["sizes_processed"].append(size_summary)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            if manager.should_stop_testing():
                overall_results["stopped_early"] = True
                overall_results["stop_reason"] = "Neo4j –ø–æ–∫–∞–∑–∞–ª —è–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–∞ –¥–≤—É—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö"
                log.info("=" * 80)
                log.info("üö® –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–°–¢–ê–ù–û–í–õ–ï–ù–û –ü–û –£–°–õ–û–í–ò–Æ")
                log.info("üìä –ü—Ä–∏—á–∏–Ω–∞: %s", overall_results["stop_reason"])
                break
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    overall_results["end_time"] = datetime.now().isoformat()
    overall_results["total_sizes_processed"] = len(overall_results["sizes_processed"])
    
    with open(overall_results_file, "w", encoding="utf-8") as f:
        json.dump(overall_results, f, ensure_ascii=False, indent=2)
    
    log.info("=" * 80)
    log.info("üèÅ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    log.info("üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–∑–º–µ—Ä–æ–≤: %d", overall_results["total_sizes_processed"])
    log.info("üìà –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –¥–æ—Å—Ä–æ—á–Ω–æ: %s", 
            "–î–ê" if overall_results["stopped_early"] else "–ù–ï–¢")
    log.info("üíæ –û–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: %s", overall_results_file)
    
    # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É
    log.info("\nüìã –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê:")
    for i, size_info in enumerate(overall_results["sizes_processed"]):
        efficiency = size_info.get("efficiency_summary", {})
        log.info("  %d. %s: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å %.2fx, Neo4j —Å—Ç–∞–±–∏–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ: %s",
                i + 1,
                size_info["size"],
                efficiency.get("average_efficiency", 1.0),
                "‚úÖ" if efficiency.get("is_neo4j_consistently_faster", False) else "‚ùå")


if __name__ == "__main__":
    main()
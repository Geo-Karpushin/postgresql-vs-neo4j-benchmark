#!/usr/bin/env python3
"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤.
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è:
 - –≤—ã–Ω–µ—Å—ë–Ω loader –≤ scripts/load_data.py
 - –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤
 - retry –¥–ª—è docker-–∫–æ–º–∞–Ω–¥
 - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ results/
"""

import subprocess
import sys
import time
import json
import logging
from pathlib import Path
from typing import List, Optional

DATA_DIR = Path("generated")
SCRIPTS_DIR = Path("scripts")
RESULTS_DIR = Path("results")
POSTGRES_CONTAINER = "database-benchmark-postgres-1"
NEO4J_CONTAINER = "database-benchmark-neo4j-1"
DOCKER_RETRIES = 4
DOCKER_BACKOFF = 2

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
log = logging.getLogger("dataset_manager")

def run_cmd(cmd: List[str], capture: bool = True, check: bool = True) -> subprocess.CompletedProcess:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥ (–±–µ–∑ shell=True)."""
    return subprocess.run(cmd, text=True, capture_output=capture, check=check)

def stream_cmd(cmd: List[str]) -> int:
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥—ã —Å realtime-–≤—ã–≤–æ–¥–æ–º stdout/stderr."""
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1
    )

    assert process.stdout is not None
    for line in process.stdout:
        print(line.rstrip())

    process.wait()
    return process.returncode

def retry_cmd(cmd: List[str], retries: int = DOCKER_RETRIES, backoff: int = DOCKER_BACKOFF) -> bool:
    """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–º–∞–Ω–¥—É —Å retry/backoff. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –ø—Ä–∏ —É—Å–ø–µ—Ö–µ, False –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ."""
    attempt = 0
    delay = backoff
    while attempt < retries:
        try:
            run_cmd(cmd)
            return True
        except subprocess.CalledProcessError as e:
            attempt += 1
            log.warning("–ö–æ–º–∞–Ω–¥–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å (–ø–æ–ø—ã—Ç–∫–∞ %d/%d): %s ‚Äî %s", attempt, retries, " ".join(cmd), e.stderr.strip()[:200])
            if attempt < retries:
                time.sleep(delay)
                delay *= 2
    return False

class DatasetManager:
    def __init__(self, dry_run: bool = False):
        self.base_path = DATA_DIR
        self.scripts_path = SCRIPTS_DIR
        self.results_path = RESULTS_DIR
        self.dry_run = dry_run
        self.results_path.mkdir(parents=True, exist_ok=True)

    def _ensure_dataset_files(self, size: str) -> bool:
        users = self.base_path / size / "users.csv"
        friendships = self.base_path / size / "friendships.csv"
        if not users.exists():
            log.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª: %s", users)
            return False
        if not friendships.exists():
            log.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª: %s", friendships)
            return False
        return True

    def initialize_databases(self) -> bool:
        log.info("üóÉÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
        try:
            run_cmd([sys.executable, str(self.scripts_path / "init_database.py")])
            log.info("‚úÖ –°—Ö–µ–º—ã –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: %s", e.stderr.strip())
            return False

    def generate_dataset(self, size: str) -> bool:
        log.info("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ %s...", size)
        try:
            subprocess.run(
                [sys.executable, str(self.scripts_path / "data_generator.py"), size],
                check=True
            )
            log.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç %s —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω", size)
            return True
        except subprocess.CalledProcessError as e:
            log.error("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
            return False

    def copy_to_containers(self, size: str) -> bool:
        log.info("üì¶ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ %s –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã...", size)
        if not self._ensure_dataset_files(size):
            return False

        users_host = str(self.base_path / size / "users.csv")
        friends_host = str(self.base_path / size / "friendships.csv")

        cp_pg_users = ["docker", "cp", users_host, f"{POSTGRES_CONTAINER}:/tmp/users.csv"]
        cp_pg_friends = ["docker", "cp", friends_host, f"{POSTGRES_CONTAINER}:/tmp/friendships.csv"]

        neo4j_dir = f"/var/lib/neo4j/import/{size}"
        mkdir_neo = ["docker", "exec", NEO4J_CONTAINER, "mkdir", "-p", neo4j_dir]
        cp_neo_users = ["docker", "cp", users_host, f"{NEO4J_CONTAINER}:{neo4j_dir}/users.csv"]
        cp_neo_friends = ["docker", "cp", friends_host, f"{NEO4J_CONTAINER}:{neo4j_dir}/friendships.csv"]

        steps = [
            (mkdir_neo, "–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ Neo4j"),
            (cp_pg_users, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ users -> Postgres"),
            (cp_pg_friends, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ friendships -> Postgres"),
            (cp_neo_users, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ users -> Neo4j"),
            (cp_neo_friends, "–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ friendships -> Neo4j"),
        ]

        for cmd, desc in steps:
            log.info("  ‚Ä¢ %s: %s", desc, " ".join(cmd) if self.dry_run else "")
            if self.dry_run:
                continue
            ok = retry_cmd(cmd)
            if not ok:
                log.error("  ‚ùå –û—à–∏–±–∫–∞ —à–∞–≥–∞: %s", desc)
                return False

        log.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç %s —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã", size)
        return True

    def load_to_databases(self, size: str) -> bool:
        log.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ %s –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –±–∞–∑—ã...", size)

        loader = self.scripts_path / "load_data.py"
        if not loader.exists():
            log.error("–°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", loader)
            return False

        try:
            subprocess.run(
                [sys.executable, str(loader), size],
                check=True
            )
            log.info("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –±–∞–∑—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True

        except subprocess.CalledProcessError:
            log.error("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
            return False

    def run_benchmarks(self, size: str) -> bool:
        log.info("üöÄ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è %s...", size)

        runner = self.scripts_path / "benchmark_runner.py"
        if not runner.exists():
            log.error("–°–∫—Ä–∏–ø—Ç –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", runner)
            return False

        try:
            subprocess.run(
                [sys.executable, str(runner), size],
                check=True
            )
            log.info("‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è %s –∑–∞–≤–µ—Ä—à–µ–Ω—ã", size)
            return True

        except subprocess.CalledProcessError:
            log.error("‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤")
            return False

    def process_size(self, size: str) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞: –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ —à–∞–≥–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."""
        result = {"size": size, "timestamps": {}, "durations": {}, "status": "unknown"}

        start = time.time()
        t0 = time.time()
        ok = self.generate_dataset(size)
        result["timestamps"]["generate_start"] = t0
        result["durations"]["generate"] = time.time() - t0
        if not ok:
            result["status"] = "generate_failed"
            return result

        t1 = time.time()
        ok = self.copy_to_containers(size)
        result["timestamps"]["copy_start"] = t1
        result["durations"]["copy"] = time.time() - t1
        if not ok:
            result["status"] = "copy_failed"
            return result

        t2 = time.time()
        ok = self.load_to_databases(size)
        result["timestamps"]["load_start"] = t2
        result["durations"]["load"] = time.time() - t2
        if not ok:
            result["status"] = "load_failed"
            return result

        t3 = time.time()
        ok = self.run_benchmarks(size)
        result["timestamps"]["benchmark_start"] = t3
        result["durations"]["benchmark"] = time.time() - t3
        if not ok:
            result["status"] = "bench_failed"
            return result

        result["status"] = "ok"
        result["total_time"] = time.time() - start
        return result

def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python dataset_manager.py [small / medium / large / x-large / xx-large] [--dry-run]")
        return

    target = sys.argv[1]
    dry = "--dry-run" in sys.argv

    manager = DatasetManager(dry_run=dry)

    if not manager.initialize_databases():
        log.error("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º –Ω–µ —É–¥–∞–ª–∞—Å—å. –í—ã—Ö–æ–¥.")
        return

    sizes = ["small", "medium", "large", "x-large", "xx-large"] if target == "all" else [target]

    for size in sizes:
        log.info("=" * 60)
        log.info("üéØ –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–¢–ê–°–ï–¢–ê: %s", size.upper())
        res = manager.process_size(size)
        out_file = manager.results_path / f"{size}.json"
        with open(out_file, "w", encoding="utf-8") as f:
            json.dump(res, f, ensure_ascii=False, indent=2)
        log.info("–°–æ—Ö—Ä–∞–Ω—ë–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç: %s", out_file)
        if res["status"] != "ok":
            log.warning("–û–±—Ä–∞–±–æ—Ç–∫–∞ %s –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å —Å—Ç–∞—Ç—É—Å–æ–º: %s", size, res["status"])
        else:
            log.info("üéâ %s –¥–∞—Ç–∞—Å–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω! (–≤—Ä–µ–º—è %.2fs)", size, res["total_time"])


if __name__ == "__main__":
    main()
